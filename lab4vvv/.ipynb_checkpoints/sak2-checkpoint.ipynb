{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Clustering and Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: (netid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due: February 16, 2017 12:00 AM [This is when Wednesday transitions to Thursday]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What You Will Need to Know For This Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-means clustering\n",
    "* Vector Quantization\n",
    "* Nearest Neighbors Classification\n",
    "* Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble (don't change this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "from numpy import genfromtxt\n",
    "import scipy.spatial.distance as dist\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from PIL import Image\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Selecting the number of clusters (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which implements K-means clustering. \n",
    "\n",
    "You will be given as input:\n",
    "* A $(N,d)$ numpy.ndarray of unlabeled data (with each row as a feature vector), data\n",
    "* A scalar $K$ which indicates the number of clusters\n",
    "* A scalar representing the number of iterations, niter (this is your stopping criterion/criterion for convergence)\n",
    "\n",
    "Your output will be a tuple consisting of a vector of length N containing which cluster ($0,\\ldots,K-1$) a feature vector is in and a $(K,d)$ matrix with the rows containing the cluster centers. \n",
    "\n",
    "Do not use scikit-learn or similar for implement K-means clustering. You may use `scipy.spatial.distance.cdist` to calculate distances. Initialize the centers randomly without replacement with points from the data set. `random.sample` may be useful for this. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kMeans(data,K,niter):\n",
    "    #Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-means clustering problem tries to minimize the following quantity by selecting $\\{z_i\\}_{i=1}^N$ and $\\{\\mu_k\\}_{k=1}^K$:\n",
    "$$J_K(\\{z_i\\}_{i=1}^N ,\\{\\mu_k\\}_{k=1}^K)=\\sum_{i=1}^N \\lVert \\mathbf{x}_i - \\mathbf{\\mu}_{z_i} \\rVert^2$$\n",
    "where $\\mathbf{\\mu}_{z_i}$ is the center of the cluster to which $\\mathbf{x}_i$ is assigned.\n",
    "\n",
    "One visual heuristic to choose the number of clusters from the data (where the number of clusters is not known a priori) is to estimate the optimal value of $J_K(\\{z_i\\}_{i=1}^N ,\\{\\mu_k\\}_{k=1}^K)$, $J^*(K)$ , for different values of $K$ and look for an \"elbow\" or \"knee\" in the curve of $J^*$ versus $K$ and choose that value of $K$. \n",
    "\n",
    "In this part of the problem, you will run $K$-means 100 times for each $K=2,\\ldots,10$ and calculate $J_K(\\{z_i\\}_{i=1}^N ,\\{\\mu_k\\}_{k=1}^K)$ for the clustering given by $K$-means. Use the smallest value of $J_K(\\{z_i\\}_{i=1}^N ,\\{\\mu_k\\}_{k=1}^K)$ in the runs of $K$-means for each value of $K$ to form an estimate of $J^*(K)$. Plot this estimate versus $K$. Which $K$ should you pick by this heuristic? Use niter=100 for each run of $K$-means.\n",
    "\n",
    "For an attempt to formalize this heuristic, see Tibshirani, Robert, Guenther Walther, and Trevor Hastie. \"Estimating the number of clusters in a data set via the gap statistic.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology) 63.2 (2001): 411-423. Sometimes, an elbow does not exist in the curve or there are multiple elbows or the $K$ value of an elbow cannot be unambiguously identified. Further material can be found on <a href=\"http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method\">Wikipedia</a> as well.  \n",
    "\n",
    "Note: Your code should be relatively quick -- a few minutes, at worst. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load up some data, which we will store in a variable called problem1\n",
    "problem1= genfromtxt('problem1.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the value of $K$ you determined from the elbow, perform K-means clustering on the data. \n",
    "Plot it as a scatter plot with the colors given by the labels. <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should you pick the $K$ such that $J^*(K)$ is minimized? Why or why not? <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Vector Quantization (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will implement vector quantization. You will use `sklearn.cluster.KMeans` for the K-means implementation and use k-means++ as the initialization method. See Section 4.2.1 in the notes for details. \n",
    "\n",
    "Write a function to generate a codebook for vector quantization. You will be given inputs:\n",
    "* A $(N,M)$ numpy.ndarray representing a greyscale image, called image. (If we want to generate our codebook from multiple images, we can concatenate the images before running them through this function).\n",
    "* A scalar $B$, for which you will use $B \\times B$ blocks for vector quantization. You may assume $N$ and $M$ are divisible by $B$.\n",
    "* A scalar $K$, which is the size of your codebook\n",
    "\n",
    "You will return:\n",
    "* The codebook as a $(K,B^2)$ numpy.ndarray. \n",
    "<b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trainVQ(image,B,K):\n",
    "    # Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function which compresses an image against a given codebook. You will be given inputs:\n",
    "* A $(N,M)$ numpy.ndarray representing a greyscale image, called image. You may assume $N$ and $M$ are divisible by $B$.\n",
    "* A $(K,B^2)$ codebook called codebook\n",
    "* $B$\n",
    "\n",
    "You will return:\n",
    "* A $(N/B,M/B)$ numpy.ndarray consisting of the indices in the codebook used to approximate the image. \n",
    "\n",
    "You can use the nearest neighbor classifier from scikit-learn if you want (though it is not necessary) to map blocks to their nearest codeword. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compressImg(image, codebook,B):\n",
    "    #Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to reconstruct an image from its codebook. You will be given inputs:\n",
    "* A $(N/B,M/B)$ numpy.ndarray containing the indices of the codebook for each block called indices\n",
    "* A codebook as a $(K,B^2)$ numpy.ndarray called codebook\n",
    "* $B$\n",
    "\n",
    "You will return a $(N,M)$ numpy.ndarray representing the image. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decompressImg(indices, codebook,B):\n",
    "    #Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run your vector quantizer with $5 \\times 5$ blocks on the provided image with codebook sizes $K=2,5,10,20,50,100,200$ (i.e. generate codebooks from this image of those sizes, compress the image using those codebooks and reconstruct the images). Display and comment on the reconstructed images (you may be quantitative (e.g. PSNR) or qualitative). Which code book would you pick? Why? Make sure to take into account the bits per pixel used by the compressor.\n",
    "\n",
    "Note the number of bits per pixel can be approximated as $\\frac{\\log_2 K}{25}$ and the codebook takes approximately $200K$ bits (assuming each pixel is stored as 8 bits). Some good ideas on quantitative arguments for codebook size can be found in Gonzalez & Woods, Digital Image Processing 3e or Gersho & Gray, Signal Compression & Vector Quantization. It is not necessary to look at these references for quantitative arguments, though. <b>(10 points)</b>\n",
    "\n",
    "The image used is under fair use from [Bleacher Report](http://bleacherreport.com/articles/2688697-tom-brady-comments-on-friendship-with-matt-ryan-ahead-of-super-bowl-51)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The provided image is stored in image\n",
    "image = np.asarray(Image.open(\"mrtb.jpg\").convert(\"L\"))\n",
    "imshow(image, cmap = cm.Greys_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Using K-means to Accelerate Nearest Neighbors (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will use K-means clustering to accelerate nearest neighbors, as outlined in the notes (Algorithm 7). Use `sklearn.neighbors.KNeighborsClassifier` for nearest neighbor classification and `sklearn.cluster.KMeans` for the K-means implementation with k-means++ as the initialization method.\n",
    "\n",
    "You will write a function to generate prototypes from labeled data. It will have input:\n",
    "* Training features as $(N,d)$ numpy.ndarray called traindata\n",
    "* Training labels as a length $N$ vector called trainlabels\n",
    "* $K$, the number of prototypes under each class\n",
    "\n",
    "You will return a tuple containing:\n",
    "* The prototypes selected as a $(K*\\text{number of classes},d)$ numpy.ndarray\n",
    "* The corresponding labels as a $K*\\text{number of classes}$ length vector \n",
    "\n",
    "You may assume there are at least $K$ examples under each class. `set(trainlabels)` will give you the set of labels. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generatePrototypes(traindata,trainlabels,K):\n",
    "    # Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a nearest neighbor classifier (i.e. 1-NN)  with 1,10,50,100 and 200 prototypes per class for the digits data set from Lab 2. Comment on the validation error and computational complexity versus the nearest neighbor classifier from Lab 2 (error=0.056) and the LDA classifier (error=0.115) from Lab 2. Which classifier would you pick? Why? \n",
    "\n",
    "Note that this data set is generated from zip code digits from US mail, and the US Postal Service processes <a href=\"https://about.usps.com/who-we-are/postal-facts/one-day-by-the-numbers.htm\">hundreds of millions of pieces of mail</a> a day, so a small improvement in error can lead to tremendous savings in terms of mis-routed packages (which cost a lot of money and time to re-transport). <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the digits data set\n",
    "\n",
    "#Read in the Training Data\n",
    "traindata_tmp= genfromtxt('zip.train', delimiter=' ')\n",
    "#The training labels are stored in \"trainlabels\", training features in \"traindata\"\n",
    "trainlabels=traindata_tmp[:,0]\n",
    "traindata=traindata_tmp[:,1:]\n",
    "\n",
    "\n",
    "#Read in the Validation Data\n",
    "valdata_tmp= genfromtxt('zip.val', delimiter=' ')\n",
    "#The validation labels are stored in \"vallabels\", validation features in \"valdata\"\n",
    "vallabels=valdata_tmp[:,0]\n",
    "valdata=valdata_tmp[:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Problem 4: Linear Regression (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, you will do model selection for linear regression using Ordinary Least Squares, Ridge Regression and the LASSO.\n",
    "\n",
    "The dataset you will use has 8 features:\n",
    "\n",
    "    lcavol - log cancer volume\n",
    "    lcaweight - log prostate weight\n",
    "    age\n",
    "    lbph - log of amount of benign prostatic hyperplasia\n",
    "    svi - seminal vesicle invasion\n",
    "    lcp - log capsular penetration\n",
    "    gleason - Gleason score\n",
    "    pgg45 - percent of Gleason scores 4 or 5\n",
    "\n",
    "and you will predict the level of a prostate-specific antigen. The data set was collected from a set of men about to receive a radical prostatectomy. More details about this dataset are given in Section 3.2.1 in Elements of Statistical Learning 2e by Hastie et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "trainp= genfromtxt('trainp.csv', delimiter=',')\n",
    "\n",
    "# Training data: \n",
    "trainfeat=trainp[:,:-1] #Training features (rows are feature vectors)\n",
    "trainresp=trainp[:,-1] #Training responses\n",
    "\n",
    "valp= genfromtxt('valp.csv',delimiter=',')\n",
    "# Validation data:\n",
    "valfeat=valp[:,:-1] #Validation Features (rows are feature vectors)\n",
    "valresp=valp[:,-1] #Validation Response\n",
    "\n",
    "# Standardize and center the features\n",
    "ftsclr=StandardScaler()\n",
    "trainfeat = ftsclr.fit_transform(trainfeat)\n",
    "valfeat= ftsclr.transform(valfeat)\n",
    "# and the responses (note that the example in the notes has centered but not \n",
    "#                    standardized responses, so your numbers won't match up)\n",
    "rsclr=StandardScaler()\n",
    "trainresp = (rsclr.fit_transform(trainresp.reshape(-1,1))).reshape(-1)\n",
    "valresp= (rsclr.transform(valresp.reshape(-1,1))).reshape(-1)\n",
    "\n",
    "# The training features are in trainfeat\n",
    "# The training responses are in trainresp\n",
    "# The validation features are in valfeat\n",
    "# The validation responses are in valresp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we centered the responses, we can begin with a benchmark model: Always predict the response as zero (mean response on the training data). Calculate the validation RSS for this model. **(5 points)**\n",
    "\n",
    "If another model does worse than this, it is a sign that something is amiss.\n",
    "\n",
    "Note: The RSS on a data set with $V$ samples is given by $\\frac{1}{V} \\lVert \\mathbf{y} - \\hat{\\mathbf{y}} \\rVert^2$ where $\\mathbf{y}$ is a vector of the responses, and $\\hat{\\mathbf{y}}$ is the predicted responses on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you will try (Ordinary) Least Squares. Use `sklearn.linear_model.LinearRegression` with the default options. Calculate the validation RSS. <b>(5 points)</b>\n",
    "\n",
    "Note: The .score() method returns an [$R^2$  value](https://en.wikipedia.org/wiki/Coefficient_of_determination), not the RSS, so you shouldn't use it anywhere in this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will apply ridge regression with `sklearn.linear_model.Ridge`. \n",
    "\n",
    "Sweep the regularization/tuning parameter $\\alpha=0,\\ldots,100$ with 1000 equally spaced values. \n",
    "\n",
    "Make a plot of the RSS on the validation set versus $\\alpha$. What is the minimizing $\\alpha$, corresponding coefficients and validation error? \n",
    "\n",
    "Larger values of $\\alpha$ shrink the weights in the model more. $\\alpha=0$ corresponds to the LS solution. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will apply the LASSO with `sklearn.linear_model.Lasso`. \n",
    "\n",
    "Sweep the tuning/regularization parameter $\\alpha=0,\\ldots,1$ with 1000 equally spaced values. \n",
    "\n",
    "Make a plot of the RSS on the validation set versus $\\alpha$. What is the minimizing $\\alpha$, corresponding coefficients and validation error? \n",
    "\n",
    "\n",
    "Larger values of $\\alpha$ lead to sparser solutions (i.e. less features used in the model), with a sufficiently large value of $\\alpha$ leading to a constant prediction. Small values of $\\alpha$ are closer to the LS solution, with $\\alpha=0$ being the LS solution. <b>(10 points)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert Answer Here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which features were selected by Ridge Regression when minimizing the RSS on the validation set? Which features were selected by LASSO when minimizing the RSS on the validation set? Which model would you choose (and why)? <b>(5 points)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[Insert Answer Here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
