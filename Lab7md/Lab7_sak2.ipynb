{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please submit this file only, with all results on it. Do not print intermeidia results, like print(image)!\n",
    "\n",
    "Name:\n",
    "\n",
    "NetID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.fftpack import dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short Time Fourier Transform. Overlap 50%\n",
    "\n",
    "1. Write a function that will compute a STFT of a single channel of audio. The spectrogram numpy array is already preallocated for you. \n",
    "2. Read in a portion of the audio files provided, plot your results using the provided `plotSTFT()` function. (If you did this part correctly, you should be getting a plot very similar to what you obtain from `specgram()` of last week's lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stft(x, N = 256, M = 128):\n",
    "    ## this function should take in a signal x, \n",
    "    ## and compute a N-point STFT with step size M\n",
    "    ## with a Hamming window applied to it\n",
    "\n",
    "    n = len(x) # total length of the signal x\n",
    "    K = (n-N)/M + 1 # number of  frames. The step between frames is M. \n",
    "    N2 = N/2  # keep only the positive frequencies. \n",
    "    S = np.zeros([N2+1,K+1]) # spectrogram of N2 + 1 frequencies and K + 1 time indices\n",
    "    \n",
    "    ## Your Code here\n",
    "   \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotSTFT(S):\n",
    "    N = S.shape[0] # number of frequencies points\n",
    "    tt = np.arange(S.shape[1]) # number of time indices\n",
    "    freq = np.hstack([np.arange(0., N)/N, 1]) # frequency axis\n",
    "    plt.pcolormesh(tt, freq, 10 * np.log10(S[:,:-1]+ 0.001), cmap = 'jet')  # plot your spectrogram\n",
    "    plt.axis('tight')\n",
    "    plt.ylabel('frequency (normalized)')\n",
    "    plt.xlabel('time (in samples)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Fs, s = wavfile.read('data/italian.wav')\n",
    "\n",
    "chunk_audio = s[100000:200000]\n",
    "S = stft(chunk_audio)\n",
    "\n",
    "plt.figure(figsize = (16, 5))\n",
    "plotSTFT(S) \n",
    "plt.title('My Spectrogram of Audio Signal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel Frequency Cepstrum\n",
    "\n",
    "The Mel Frequency Ceptrum is a human auditory response insipred audio feature that is being used succesfully in many audio processing applications. The Mel Frequency Cepstral Coefficients ([MFCC](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)) are computed as follows:\n",
    "\n",
    "1. Compute the Fourier Transform of the signal\n",
    "2. Map power spectrum to the mel scale, using overlapping triangular windows (filter banks). \n",
    "3. Take log of the power spectrum in the mel 'frequency'\n",
    "4. Compute the Discrete Cosine Transform of the spectrum computed above, as though it were a signal\n",
    "\n",
    "The amplitudes obtained are the MFCC. A block diagram of the process is shown here:\n",
    "![](MFCC.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement each components below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.  Converting from frequency to mel frequency and vice versa\n",
    "\n",
    "First, two important formulas:\n",
    "\n",
    "Formula to convert frequency $f$ Hertz to $m$ Mel:\n",
    "\n",
    "$$m = 2595 * log_{10}(1+\\frac{f}{700})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "1. Derive the formula to convert from Mel back to Hertz\n",
    "2. Implement these 2 conversions as functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hint: \n",
    "\n",
    "see [this](http://stackoverflow.com/questions/13208286/how-to-write-latex-in-ipython-notebook) on how to write equations in the notebook and [this](https://wch.github.io/latexsheet/) for Latex cheat sheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hz2mel(h):\n",
    "    ## This function should output the Mel quefrency given some input frequency\n",
    "    ## h is a 1-d array of input frequencies in hertz, \n",
    "    ## Your code here\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def mel2hz(m):\n",
    "    ## This function should output the frequency in Hertz given some input quefrency\n",
    "    ## m is a 1-d array of input quefencies in Mel, \n",
    "    ##Your code here\n",
    "    \n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Mel-spaced filter banks\n",
    "\n",
    "The Mel-spaced filterbank is the set of triangular window filter which corresponds to [human hearing perception](https://en.wikipedia.org/wiki/Mel_scale).\n",
    "![](melfilterbank.jpg)\n",
    "The following code is to use to generate **nfilt** filters using the end points supplied by binPoints vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter bank generation\n",
    "def filterBank(nfilt, NFFT, binPoints):\n",
    "    fbank = np.zeros([nfilt,NFFT/2+1])\n",
    "    for j in xrange(0,nfilt):\n",
    "        for i in xrange(int(binPoints[j]),int(binPoints[j+1])):\n",
    "            fbank[j,i] = (i - binPoints[j])/(binPoints[j+1]-binPoints[j]) #rising edge of the triangle\n",
    "        for i in xrange(int(binPoints[j+1]),int(binPoints[j+2])):\n",
    "            fbank[j,i] = (binPoints[j+2]-i)/(binPoints[j+2]-binPoints[j+1]) #falling edge of the triangle\n",
    "    return fbank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at how the Mel-spaced filter banks are applied to the power spectrum of an audio signal. The following code generates the filter banks given the sampling frequency, number of filters, and length of the FFT. (You will need to finish the code for previous section for this part to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Fs, x = wavfile.read('data/WakeMeUp.wav')     # x stores the input audio signal, Fs is the sampling frequency\n",
    "signal = x[:,0]\n",
    "\n",
    "highfreq = Fs/2       # Nyquist rate\n",
    "lowfreq = 0           # lower end of audio spectrum\n",
    "\n",
    "lowmel = hz2mel(lowfreq)     # convert to Mel scales\n",
    "highmel = hz2mel(highfreq)\n",
    "\n",
    "NFFT = 512            # number of FFT points\n",
    "nfilt = 20            # number of Mel spaced filter banks\n",
    "\n",
    "# convert the frequency into Mel points\n",
    "melPoints = np.linspace(lowmel, highmel, nfilt + 2) # linear in Mel scale correspondings to human perception of sound\n",
    "binPoints = np.floor(mel2hz(melPoints)/Fs*(NFFT+1)) # convert back to Hert \n",
    "\n",
    "fbank = filterBank(nfilt, NFFT, binPoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Why do we need nfilt + 2 points for the melPoints array?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a visualization of the 20 filter banks in the frequency domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq = np.arange(257)*(Fs/2.)/257    # generate frequencies for plot\n",
    "plt.figure(figsize = (12, 9)) # make a bigger figure\n",
    "for i in np.arange(nfilt):\n",
    "    plt.plot(freq, fbank[i,:]) # plot 20 filter banks\n",
    "plt.title('Filter Bank Responses')\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel('frequency')\n",
    "ax.set_ylabel('amplitude')\n",
    "ax.set_xlim([0, 22050]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: looking at the filter bank, are human hearing sensitive to low or high frequencies?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the power spectrum of a window of the audio signal. The plot is of what would be one column of the spectrogram as created in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = 10000 # 10000 samples\n",
    "s = signal[T:T+NFFT]\n",
    "f =  np.square((1.0/NFFT) *np.absolute(np.fft.rfft(s,)))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(freq, f)\n",
    "plt.title('Power Spectrum (PS) of Signal')\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel('frequency')\n",
    "ax.set_ylabel('magnitude')\n",
    "ax.set_xlim([0, 22050])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C.  Filter bank coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we see the result of applying the 12th filter bank to the window of the audio signal. To apply the filter, the filter and the power spectrum of the signal are simply multiplied together. The plot below is the product of the filter and the PS of the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_f = fbank*np.tile(f, (nfilt, 1))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(freq, filtered_f[11,:])\n",
    "plt.title('PS of Signal Filtered by 12th Filter Bank')\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel('frequency')\n",
    "ax.set_ylabel('magnitude')\n",
    "ax.set_xlim([0, 22050])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Plot the result of the 4th filter bank applied to the windowed signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what we will need for features are actually are the inner products of the power spectrum with the filters, and not the multiplication, thus creating *coefficients*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2914797.01716018  1597537.80079218   700419.69833245   316521.46870314\n",
      "   625488.47139375    91069.88281031   111086.84836453   134491.58309571\n",
      "    62539.07656345    63745.50818129    34571.24814318    56992.78716138\n",
      "    38742.49989657    51363.64651115    50001.70737522    26471.24847627\n",
      "    16122.10005486     6225.27937841     3986.19942855     4003.2560767 ]\n"
     ]
    }
   ],
   "source": [
    "fbank_coefs = np.dot(fbank,f)\n",
    "print fbank_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: what are the dimensions of fbank and f?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Generating the MFCC's\n",
    "\n",
    "The MFCC's are the [DCT](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.fftpack.dct.html) of the **log** of the filter bank coefficients. For the windowed input audio signal, the coefficients are computed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "melMapped = np.log(fbank_coefs) # take the log of the mel-spaced mapping signal\n",
    "mfcc = dct(melMapped.T, type = 2, axis = 0, norm = 'ortho')# take the Discrete Cosine Transform\n",
    "\n",
    "plt.plot(mfcc)\n",
    "plt.title('MFCC of window of \"WakeMeUp.wav\"')\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel('frequency bin')\n",
    "ax.set_ylabel('value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the coefficients of a window of 'WakeMeUp.wav' to a window of a different file, 'chopin.wav,' of a different genre of music with different frequency content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Fs2, x2 = wavfile.read('data/chopin.wav')     # x stores the input audio signal, Fs is the sampling frequency\n",
    "signal2 = x2[:,0]\n",
    "\n",
    "T = 10000\n",
    "w2 = np.hamming(N)      # generate a Hamming window of length 300\n",
    "s2 = signal2[T:T+NFFT]\n",
    "f2 =  np.square((1.0/NFFT) *np.absolute(np.fft.rfft(s2,)))\n",
    "\n",
    "fbank_coefs2 = np.dot(fbank,f2)\n",
    "\n",
    "melMapped2 = np.log(fbank_coefs2) # take the log of the mel-spaced mapping signal\n",
    "mfcc2 = dct(melMapped2.T, type = 2, axis = 0, norm = 'ortho')# take the Discrete Cosine Transform\n",
    "\n",
    "plt.plot(mfcc2)\n",
    "plt.title('MFCC of window of \"chopin.wav\"')\n",
    "ax = plt.axes()\n",
    "ax.set_xlabel('frequency bin')\n",
    "ax.set_ylabel('value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "With the information and code examples provided above, you should now be able to implement a function to compute the MFCC of a given signal. (Hint, you will need the functions that you have defined so far, and other crucial code components are already provided above). You divide your signal into frames of length N, calculate the MFCC of that frame (which should give you 12 MFCC coefficients). The return variable of the function should have dimension of numberOfFrames by 12, where numberOfFrames is the number of frames in the signal **data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcMFCC(Fs, data, N, M):\n",
    "    #Your code here\n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: look at the file mfcclab.py to find the number of frames in a signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Identification\n",
    "\n",
    "In this exercise, we will utilize the MFCC feature to recognize the identity of a speaker. You should put the two folders that were provided, train and test, into the same folder of your ipython notebook. There are 8 training data files and 8 testing data files, s1.wav to s8.wav. The training and testing data are already correctly labeled, i.e, s1.wav in the training folder matches with s1.wav in the testing folder (same person speaking). Nearest neighbor will be used to label the testing data.\n",
    "\n",
    "Each audio file in this dataset is mono-channel, thus you do not need to split the files into two channels as in the previous part. However the files are of different length, which requires special attention to allocate the training dataset. For each audio file in the train folder, we create its MFCC matrix, where each row corresponds to 12 MFCC's of one frame. If your signal has 125 frames, for instance, the size of MFCC matrix will be 125 by 12. The MFCC matrix of all audio files are stacked into one matrix called the **codebook**. At the same time, we also store the labels of the training data. \n",
    "\n",
    "## Training\n",
    "\n",
    "We first calculate the total number of frames of our whole dataset. This is used to initialize the codebook matrix and the label vector. Let's say you have only 2 audio files. The first file has 10 frames, and the second has 20 frames. Then the size of your **codebook** is 30 by 12. \n",
    "\n",
    "The length of your **labels** vector will be 30, in which the first 10 elements are *1* and the last 20 elements are *2*. For this exercise, we will choose the length of each frame to be 512 and the step size is one third of the frame length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mfcclab\n",
    "# find the number of frames in advance\n",
    "# this is to reduce dynamic array allocation \n",
    "# which slows down our code\n",
    "N = 512\n",
    "M = 512/3\n",
    "sumNumFrames = mfcclab.get_num_frames(\"train\", N, M) # total number of frames for all of the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the range of frequencies for our filter banks in Hertz, *lowfreq* and *highfreq* (from 0 Hz to the Nyquist rate). To create the endpoints for our filter banks, we transform this range into mel-scale range. The *for* loop loads all the training data in the **train** folder, computes the MFCC's, and appends them to the **codebook**. Since we know the labels of our data, this is considered an example of *supervised learning*, as opposed to an *unsupervised learning* paradigm, like K-means clustering, where the labels are unknown and must be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "highfreq = Fs/2 # Nyquist rate\n",
    "lowfreq = 0 # lower end of audio spectrum\n",
    "\n",
    "lowmel = hz2mel(lowfreq) # convert to Mel scales\n",
    "highmel = hz2mel(highfreq)\n",
    "\n",
    "NFFT = 512 # number of FFT points\n",
    "\n",
    "nfilt = 20 # number of Mel spaced filter banks\n",
    "# convert the frequency into Mel points\n",
    "melPoints = np.linspace(lowmel, highmel, nfilt + 2)\n",
    "binPoints = np.floor(mel2hz(melPoints)/Fs*(NFFT+1))\n",
    "\n",
    "numKeptCoeff = 12 # we only keep the lower 12 coefficients since the higher frequencies \n",
    "# do not help speaker identification\n",
    "fbank = filterBank(nfilt, NFFT, binPoints) # create the Mel-spaced filter bank\n",
    "codeBook = np.empty((sumNumFrames, numKeptCoeff-1)) #preallocate the training data\n",
    "labels = np.zeros((sumNumFrames,)) # and the labels\n",
    "N = 512 # lenghth of a frame\n",
    "M = 512/3 # step size\n",
    "currentIdx = 0\n",
    "for i in range(1,9): # we have 8 signals\n",
    "    filename = \"train/s%d.wav\" % i # this is nhow you replace %d with i in Python 2. \n",
    "    fs, data = wavfile.read(filename)\n",
    "    mfcc = calcMFCC(fs, data, N, M)[:,1:numKeptCoeff]\n",
    "    codeBook[currentIdx:currentIdx + mfcc.shape[0],:] = mfcc# assign this training data to codebook\n",
    "    labels[currentIdx:currentIdx + mfcc.shape[0]] = i # and label them\n",
    "    currentIdx = currentIdx + mfcc.shape[0] # move to next block in codeBook and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the each codeword in the codebook is of dimension 11, it is not possible to visualize the distribution of the codewords and their proximity to each other. However, tools exist that attempt to reduce the dimensionality of data while preserving the relative distances between data points. The following code uses a tool called Isomap (a non-linear dimension-reduction technique, unlike PCA) to plot the data in 2D to give some intuition of the distribution of the codewords. How well grouped are the codewords for each speaker?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mfcclab.plot_codebook_2D(codeBook, labels, [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"s6\", \"s7\", \"s8\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your favourite classifier from part I of the course to train and test on the given data. What is the accuracy of your classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 - Extra Credit\n",
    "\n",
    "The current algorithm will predict a speaker regardless if it is in the database. Propose and implement a modification that will be able to do so. Record a short clip of your own voice and briefly describe and comment on your results and observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In last week's lab you were asked to propose a method to find where is the sky in a picture. A more general version of this task is called image segmentation, where we attempt to split an image into regions that are the same 'thing'. In this lab, we will attempt to implement a simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do we define a segment?\n",
    "\n",
    "We can work with a simplistic model that the same 'thing' will have similar color. If we make this assumption, we can make use of clustering algorithms to attempt segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "1. Give 1 potential pitfall when using such a simplistic assumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation using K-Means\n",
    "\n",
    "As we have learnt from the previous lab, an image is merely a collection of RGB values. We can ignore the spatial arrangement of every pixel, and treat each one of them as a 3-dimensional vector. Pixels with similar RGB values will be close to each other in a 3 dimensional space.\n",
    "\n",
    "This will allow us to formulate the segmentation problem as follows:\n",
    "1. Decide on the number of segments,$k$, in the image\n",
    "2. Reshape the image into a $(w*h) \\times 3$ vector, where each row is a pixel's RGB values\n",
    "3. Run the K-Means algorithm using this as the training data. \n",
    "\n",
    "To visualize the clustering, you might want to replace all pixels in the cluster with a single color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "1. Implement the algorithm described above\n",
    "2. Try a different number of clusters (3, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
